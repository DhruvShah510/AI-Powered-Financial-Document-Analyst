{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tiny Lamma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OofwFaeASaQI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tk6ui4npVfI6",
    "outputId": "ef3cf158-8217-4ce0-ef99-513d32cc4d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /content/drive/MyDrive/models/TinyLlama_TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM and Tokenizer loaded successfully!\n",
      "✅ TinyLlama LLM Pipeline created and ready!\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Load TinyLlama LLM ---\n",
    "# This cell loads the TinyLlama model you've already saved to your Google Drive.\n",
    "# We are now properly wrapping it in a HuggingFacePipeline for LangChain.\n",
    "\n",
    "# Define the path to your saved model on Google Drive\n",
    "repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "drive_model_path = f\"/content/drive/MyDrive/models/{repo_id.replace('/', '_')}\"\n",
    "llm = None # Initialize llm to None\n",
    "\n",
    "print(f\"Loading model from: {drive_model_path}\")\n",
    "\n",
    "if not os.path.exists(drive_model_path):\n",
    "    print(\"❌ Model not found. Please run the 'colab_local_model_workflow' notebook to download and save the model first.\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(drive_model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(drive_model_path, device_map=\"auto\")\n",
    "    print(\"✅ LLM and Tokenizer loaded successfully!\")\n",
    "\n",
    "    # Create a standard pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.3,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        # return_full_text=False, # This is the key change!\n",
    "    )\n",
    "\n",
    "    # UPGRADE: Wrap the pipeline in a LangChain-compatible object for better control\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(\"✅ TinyLlama LLM Pipeline created and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMRiQvKMWrKH",
    "outputId": "ae52dc28-28eb-46d5-eb1b-4704ec53af63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF file found at: /content/drive/MyDrive/Annual report financial analyst/microsoft_2024_annual_report.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Prepare Your Document ---\n",
    "# This cell remains the same. Ensure your PDF is in Google Drive.\n",
    "\n",
    "pdf_file_path = \"/content/drive/MyDrive/Annual report financial analyst/microsoft_2024_annual_report.pdf\" # <--- EDIT THIS LINE\n",
    "\n",
    "if not os.path.exists(pdf_file_path):\n",
    "    print(f\"❌ PDF file not found at '{pdf_file_path}'.\")\n",
    "else:\n",
    "    print(f\"✅ PDF file found at: {pdf_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hRggCwcXKfZ",
    "outputId": "64fd9326-a1f8-4731-f771-6073ce966a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "PDF loaded successfully. It has 91 pages.\n",
      "Document split into 307 chunks.\n",
      "Embedding model loaded.\n",
      "Creating FAISS vector store from document chunks... This may take a moment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Load, Split, and Embed the Financial Report ---\n",
    "# This cell remains the same. It builds our knowledge base.\n",
    "\n",
    "vector_store = None # Initialize to None\n",
    "if os.path.exists(pdf_file_path) and llm:\n",
    "    print(\"Loading PDF...\")\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    pages = loader.load()\n",
    "    print(f\"PDF loaded successfully. It has {len(pages)} pages.\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    print(f\"Document split into {len(docs)} chunks.\")\n",
    "\n",
    "    embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    print(\"Embedding model loaded.\")\n",
    "\n",
    "    print(\"Creating FAISS vector store from document chunks... This may take a moment.\")\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    print(\"✅ Vector store created successfully!\")\n",
    "else:\n",
    "    print(\"Skipping RAG pipeline creation due to missing PDF or LLM.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GGnBpgh_XbQ9"
   },
   "outputs": [],
   "source": [
    "# --- Cell 5: UPGRADE - Create a LangChain Q&A Chain ---\n",
    "# This function is now much cleaner and more robust using a proper LangChain chain.\n",
    "# This is the key fix for the output quality issues.\n",
    "\n",
    "def ask_analyst(question, vector_store, llm):\n",
    "    \"\"\"\n",
    "    This function now uses a proper LLMChain for robust Q&A.\n",
    "    \"\"\"\n",
    "    if not vector_store or not llm:\n",
    "        return \"Error: Vector store or LLM not initialized.\"\n",
    "\n",
    "    print(f\"\\nAnalysing report for the question: '{question}'\")\n",
    "    relevant_docs = vector_store.similarity_search(question, k=4)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # print(\"\\n--- Context Provided to LLM ---\")\n",
    "    # print(context)\n",
    "    # print(\"-------------------------------\")\n",
    "\n",
    "    # Define a clear prompt template using the model's preferred format\n",
    "    # This structure prevents the model from getting confused and repeating the input.\n",
    "    template = \"\"\"\n",
    "<|system|>\n",
    "You are an expert financial analyst. Your task is to answer the user's question based *only* on the provided text from the company's annual report. Be precise and cite specific numbers or facts from the text. If the answer is not in the provided text, say 'The answer is not available in the provided context.'</s>\n",
    "<|user|>\n",
    "CONTEXT FROM ANNUAL REPORT:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "QUESTION: {question}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    # Create the LangChain chain\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    print(\"\\nGenerating final answer with LangChain...\")\n",
    "    # Run the chain\n",
    "    result = llm_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "    # FINAL FIX: The result['text'] contains the full prompt plus the answer.\n",
    "    # We need to parse it to get ONLY the text generated by the assistant.\n",
    "    full_text = result['text']\n",
    "\n",
    "    # Find the position of the final assistant tag\n",
    "    assistant_tag_position = full_text.rfind(\"<|assistant|>\")\n",
    "\n",
    "    # If the tag is found, slice the string from that point onwards\n",
    "    if assistant_tag_position != -1:\n",
    "        # The actual answer starts after the tag and a newline character\n",
    "        clean_answer = full_text[assistant_tag_position + len(\"<|assistant|>\"):].strip()\n",
    "    else:\n",
    "        # As a fallback, if the tag isn't there, return the whole text\n",
    "        clean_answer = full_text.strip()\n",
    "\n",
    "    return clean_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSWrelyeX6Q9",
    "outputId": "3239218a-eebb-47bb-9124-90c8c2b584f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysing report for the question: 'What were the total revenues for the most recent fiscal year?'\n",
      "\n",
      "Generating final answer with LangChain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyst's Answer 1 ---\n",
      "The total revenues for the most recent fiscal year were $198.27 billion, which is the answer provided in the context.\n",
      "\n",
      "Analysing report for the question: 'Summarize the main business risks mentioned in the report.'\n",
      "\n",
      "Generating final answer with LangChain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyst's Answer 2 ---\n",
      "The report mentions risks and uncertainties related to the following business areas:\n",
      "\n",
      "1. Financial Analysis: The company provides financial analysis services, which involve the preparation of financial statements, including annual reports. The report mentions risks and uncertainties related to the accuracy of the financial statements, including the availability of relevant information and the timeliness of the financial reporting process.\n",
      "\n",
      "2. Risk Factors: The report includes risks and uncertainties related to the company's business, including the risks and uncertainties related to the company's growth, market competition, technology innovation, and customer demand.\n",
      "\n",
      "3. Risk Factors: The report also mentions risks and uncertainties related to the company's financial performance, including the risks and uncertainties related to the company's ability to generate revenue and achieve profitability, the risks and uncertainties related to the company's ability to maintain its competitive position, and the risks and uncertainties related to the company's ability to manage its growth and expand its operations.\n",
      "\n",
      "4. Risk Factors: The report also mentions risks and uncertainties related to the company's supply chain, including the risks and uncertainties related to the availability and quality of raw materials, the risks and uncertainties related to the company's ability to manage its supply chain, and the risks and uncertainties related to the company's ability to mitigate supply chain disruptions.\n",
      "\n",
      "5. Risk Factors: The report also mentions risks and uncertainties related to the company's intellectual property, including the risks and uncertainties related to the company's ability to protect its intellectual property, the risks and uncertainties related to the company's ability to license or acquire new intellectual property, and the risks and uncertainties related to the company's ability to manage its intellectual property portfolio.\n",
      "\n",
      "6. Risk Factors: The report also mentions risks and uncertainties related to the company's regulatory environment, including the risks and uncertainties related to the company's compliance with regulatory requirements, the risks and uncertainties related to the company's ability to obtain necessary regulatory approvals, and the\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Run the Analysis! ---\n",
    "# Let's ask the same questions and see the improved results.\n",
    "\n",
    "if vector_store and llm:\n",
    "    q1 = \"What were the total revenues for the most recent fiscal year?\"\n",
    "    q2 = \"Summarize the main business risks mentioned in the report.\"\n",
    "\n",
    "    # Ask the first question\n",
    "    answer1 = ask_analyst(q1, vector_store, llm)\n",
    "    print(\"\\n--- Analyst's Answer 1 ---\")\n",
    "    print(answer1)\n",
    "\n",
    "    # Ask the second question\n",
    "    answer2 = ask_analyst(q2, vector_store, llm)\n",
    "    print(\"\\n--- Analyst's Answer 2 ---\")\n",
    "    print(answer2)\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot run analysis because the RAG pipeline was not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qhoDMAoPX97m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
